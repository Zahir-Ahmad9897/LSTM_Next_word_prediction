{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_NnmmcGiom4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgCISCMeeFUJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsDt60DvadGs"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "import pandas as pd\n",
        "# load the dataset\n",
        "data=gutenberg.raw('shakespeare-hamlet.txt')\n",
        "# save the dataset\n",
        "with open('hamlet.txt','w') as file:\n",
        "  file.write(data)\n",
        "  # import dataset in reading mode\n",
        "with open('hamlet.txt','r') as file:\n",
        "  faqs=file.read().lower()\n",
        "#  apply regular expression\n",
        "cleaned_text=re.sub('[^a-zA-Z]',' ',faqs)\n",
        "cleaned_text=re.sub(r'\\s+',' ', cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([faqs])"
      ],
      "metadata": {
        "id": "J1D42emD32Ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=len(tokenizer.word_index)+1\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "YrpAl3EDAgvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "for sentence in faqs.split('.'):\n",
        "  tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "  if len(tokenized_sentence) < 2:\n",
        "        continue\n",
        "  for i in range(1,len(tokenized_sentence)):\n",
        "    input_sequences.append(tokenized_sentence[:i+1])"
      ],
      "metadata": {
        "id": "44VahqKdAjr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences"
      ],
      "metadata": {
        "id": "UyqwPDzNA5mR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max([len(x) for x in input_sequences])"
      ],
      "metadata": {
        "id": "CrzbvUUQCXPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_input_sequences = pad_sequences(input_sequences, maxlen = max_len, padding='pre')"
      ],
      "metadata": {
        "id": "9oPMoWBSD1_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_sequences"
      ],
      "metadata": {
        "id": "miRb-QZyIi7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = padded_input_sequences[:,:-1]"
      ],
      "metadata": {
        "id": "qVI0-UUrIsd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = padded_input_sequences[:,-1]"
      ],
      "metadata": {
        "id": "lXrYHTDFI3uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "kmsFnHx1Qdow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "-wyYqYgZSeck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5OL3vrEXSs_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "y = to_categorical(y,num_classes=vocab_size)"
      ],
      "metadata": {
        "id": "rs1NPitwSgzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "iQMJ0I6xSiZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding token\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "IeIhFVJrcNv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,shuffle=True)"
      ],
      "metadata": {
        "id": "hIkc5agKtksV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "earlystoping=EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True)"
      ],
      "metadata": {
        "id": "9kVeTvR2S8Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=56))\n",
        "model.add(LSTM(150,return_sequences=True))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(vocab_size, activation='softmax'))"
      ],
      "metadata": {
        "id": "wo-OYfHpTK2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "-GGjqh7ue_Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "OxxXkrSXfIBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x,y,validation_data=(x_test,y_test),epochs=100,callbacks=[earlystoping])"
      ],
      "metadata": {
        "id": "LpFUCALCfJRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "text = \"\"\"\n",
        "ar. Horatio saies, 'tis but our Fantasie,\n",
        "And will not let beleefe take hold of him\n",
        "Touching this dreaded sight, twice seene of vs,\n",
        "Therefore I haue intreated him along\n",
        "With vs, to watch the minutes of this Night,\n",
        "That if againe this Apparition \"\"\"\n",
        "\n",
        "for i in range(10):\n",
        "  # tokenize\n",
        "  token_text = tokenizer.texts_to_sequences([text])[0]\n",
        "  # padding\n",
        "  padded_token_text = pad_sequences([token_text], maxlen=56, padding='pre')\n",
        "  # predict\n",
        "  pos = np.argmax(model.predict(padded_token_text))\n",
        "\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index == pos:\n",
        "      text = text + \" \" + word\n",
        "      print(text)\n",
        "      time.sleep(2)"
      ],
      "metadata": {
        "id": "PGeYGwCMfTus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save to your Drive path\n",
        "model.save('/content/drive/MyDrive/LSTM_Next_Word_Prediction_Model.keras')\n"
      ],
      "metadata": {
        "id": "92y7gE6pj9EZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4620d27-3fe7-43b7-d887-729812ea9ec3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir('/content/drive/MyDrive')\n"
      ],
      "metadata": {
        "id": "oSQJzCJhEzyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/tokenizer.pkl','wb') as file:\n",
        "  pickle.dump(tokenizer,file)"
      ],
      "metadata": {
        "id": "wPD6FU0cCXM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for f in os.listdir('/content/drive/MyDrive'):\n",
        "    print(f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qDoYnI2CXRd",
        "outputId": "511334fd-59dd-4866-e237-23def293df0c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1661-0.txt\n",
            "Colab Notebooks\n",
            "LSTM_Next_Word_Prediction_Model.h5\n",
            "tokenizer\n",
            "tokenizer.pkl\n",
            "LSTM_Next_Word_Prediction_Model.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "se8Si5SrCXUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4hMUutLtCXXH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}